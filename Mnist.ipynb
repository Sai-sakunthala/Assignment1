{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6AASeJ568lirjYeC5kNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sai-sakunthala/Assignment1/blob/main/Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkMwQb-AsJIA",
        "outputId": "e7e3e09c-a89b-4a00-a9f5-5d1fcd7bb83a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msai-sakunthala\u001b[0m (\u001b[33msai-sakunthala-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M3h6nZ382Jmo"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "classes = len(np.unique(Y_train))\n",
        "split_index = int(0.9 * X_train.shape[0])\n",
        "x_train_final, x_val_final = X_train[:split_index], X_train[split_index:]\n",
        "y_train_final, y_val_final = Y_train[:split_index], Y_train[split_index:]"
      ],
      "metadata": {
        "id": "Qjo5eR2QdyIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c7f421-f765-45e1-fe94-5ff3fc237c7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def input_layer(x):\n",
        "    x = np.array(x)\n",
        "    if len(x.shape) == 3:\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "    return x\n",
        "\n",
        "def sigmoid(a_x):\n",
        "    a_x = np.clip(a_x, -700, 700)\n",
        "    h_x = 1 / (1 + np.exp(-a_x))\n",
        "    return h_x\n",
        "\n",
        "def der_sigmoid(a_x):\n",
        "    sig_x = sigmoid(a_x)\n",
        "    del_sig = sig_x * (1 - sig_x)\n",
        "    return del_sig\n",
        "\n",
        "def Relu(a_x):\n",
        "    h_x = np.clip(np.maximum(0, a_x), 0, 1e4)\n",
        "    return h_x\n",
        "\n",
        "def der_Relu(a_x):\n",
        "    del_Relu = (a_x > 0).astype(float)\n",
        "    return del_Relu\n",
        "\n",
        "def tanh(a_x):\n",
        "    a_x = np.clip(a_x, -700, 700)\n",
        "    h_x = (np.exp(a_x) - np.exp(-a_x))/(np.exp(a_x) + np.exp(-a_x))\n",
        "    return h_x\n",
        "\n",
        "def der_tanh(a_x):\n",
        "    del_tanh = 1 - ((np.exp(a_x) - np.exp(-a_x))/(np.exp(a_x) + np.exp(-a_x)))**2\n",
        "    return del_tanh\n",
        "\n",
        "def softmax(a_x):\n",
        "    a_x = a_x - np.max(a_x)\n",
        "    h_x = np.exp(a_x)\n",
        "    h_x = h_x/np.sum(h_x)\n",
        "    return h_x\n",
        "\n",
        "def initialize_weights_xavier(num_neurons):\n",
        "    np.random.seed(450)\n",
        "    weights = []\n",
        "    biases = []\n",
        "    for i in range(len(num_neurons)-1):\n",
        "        W = np.random.randn(num_neurons[i+1], num_neurons[i])*np.sqrt(1 / num_neurons[i])\n",
        "        b = np.zeros((1, num_neurons[i+1]))\n",
        "        weights.append(W)\n",
        "        biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "def initialize_weights_random(num_neurons):\n",
        "    np.random.seed(450)\n",
        "    weights = []\n",
        "    biases = []\n",
        "    for i in range(len(num_neurons)-1):\n",
        "        W = np.random.randn(num_neurons[i+1], num_neurons[i])\n",
        "        b = np.zeros((1, num_neurons[i+1]))\n",
        "        weights.append(W)\n",
        "        biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "def pre_activation(h_x, W, b):\n",
        "    a_x = np.dot(W, h_x.T) + b.flatten()\n",
        "    return a_x\n",
        "\n",
        "def bce_loss_function(h_x, y):\n",
        "    h_x = np.clip(h_x, 1e-8, 1.0)\n",
        "    loss = -np.log(h_x[np.argmax(y)])\n",
        "    return loss\n",
        "\n",
        "def mse_loss_function(h_x, y):\n",
        "    loss = np.sum((h_x - y)**2)\n",
        "    return loss\n",
        "\n",
        "def forward_pass(x, y, weights, biases, activation_func, n_hidden, loss_function):\n",
        "    activations = []\n",
        "    pre_activations = []\n",
        "    for i in range(n_hidden+1):\n",
        "        a_x = pre_activation(x if i == 0 else activations[-1], weights[i], biases[i])\n",
        "        h_x = softmax(a_x) if i == n_hidden else activation_func(a_x)\n",
        "        activations.append(h_x)\n",
        "        pre_activations.append(a_x)\n",
        "    loss = loss_function(h_x, y)\n",
        "    return activations, pre_activations, loss\n",
        "\n",
        "def one_hot_encode(y, num_classes):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "def back_propagation(activations, pre_activations, weights, biases, x, y, y_hat, n_hidden, activation_deriv, loss_function):\n",
        "    del_L_a = {}\n",
        "    del_L_w = {}\n",
        "    del_L_b = {}\n",
        "    del_L_h = {}\n",
        "    for i in range(n_hidden, -1, -1):\n",
        "        if i == n_hidden:\n",
        "            if loss_function == bce_loss_function:\n",
        "                del_L_a[i] = y_hat - y\n",
        "            elif loss_function == mse_loss_function:\n",
        "                del_L_a[i] = 2 * (y_hat - y) * y_hat * (1 - y_hat)\n",
        "        if i == 0:\n",
        "            del_L_w[i] = np.dot(del_L_a[i][:, np.newaxis], x[np.newaxis, :])\n",
        "            del_L_b[i] = del_L_a[i]\n",
        "            break\n",
        "        else:\n",
        "            del_L_w[i] = np.dot(del_L_a[i][:, np.newaxis], activations[i-1][np.newaxis, :])\n",
        "        del_L_b[i] = del_L_a[i]\n",
        "        del_L_h[i-1] = np.matmul(weights[i].T, del_L_a[i])\n",
        "        del_L_a[i-1] = del_L_h[i-1]*activation_deriv(pre_activations[i-1])\n",
        "    return del_L_w, del_L_b\n",
        "\n",
        "def gradient_descent(dw, db, weights, biases, learning_rate, weight_decay):\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate*(dw[i] + weight_decay * weights[i])\n",
        "        biases[i] -= learning_rate*db[i]\n",
        "    return weights, biases\n",
        "\n",
        "def momentum_gradient(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, weight_decay):\n",
        "    u_w = {}\n",
        "    u_b = {}\n",
        "    beta = 0.9\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_u_w == {} and prev_u_b == {}:\n",
        "            u_w[i] = learning_rate*dw[i]\n",
        "            u_b[i] = learning_rate*db[i]\n",
        "        else:\n",
        "            u_w[i] = beta*prev_u_w[i] + learning_rate*dw[i]\n",
        "            u_b[i] = beta*prev_u_b[i] + learning_rate*db[i]\n",
        "        weights[i] -= u_w[i]\n",
        "        biases[i] -= u_b[i]\n",
        "    return weights, biases, u_w, u_b\n",
        "\n",
        "def nestrov_gradient(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, weight_decay):\n",
        "    u_w = {}\n",
        "    u_b = {}\n",
        "    beta = 0.9\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_u_w == {} and prev_u_b == {}:\n",
        "            u_w[i] = learning_rate*dw[i]\n",
        "            u_b[i] = learning_rate*db[i]\n",
        "        else:\n",
        "            u_w[i] = beta*prev_u_w[i] + learning_rate*dw[i]\n",
        "            u_b[i] = beta*prev_u_b[i] + learning_rate*db[i]\n",
        "        weights[i] -= u_w[i]\n",
        "        biases[i] -= u_b[i]\n",
        "    return weights, biases, u_w, u_b\n",
        "\n",
        "def rmsprop_gradient(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, weight_decay):\n",
        "    u_w = {}\n",
        "    u_b = {}\n",
        "    beta = 0.9\n",
        "    epsilon = 1e-6\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_u_w == {} and prev_u_b == {}:\n",
        "            u_w[i] = (1 - beta) * (dw[i] ** 2)\n",
        "            u_b[i] = (1 - beta) * (db[i] ** 2)\n",
        "        else:\n",
        "            u_w[i] = beta * prev_u_w[i] + (1 - beta) * (dw[i] ** 2)\n",
        "            u_b[i] = beta * prev_u_b[i] + (1 - beta) * (db[i] ** 2)\n",
        "\n",
        "        weights[i] -= learning_rate * dw[i] / (np.sqrt(u_w[i] + epsilon))\n",
        "        biases[i] -= learning_rate * db[i] / (np.sqrt(u_b[i] + epsilon))\n",
        "\n",
        "    return weights, biases, u_w, u_b\n",
        "\n",
        "def adagrad_gradient(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, weight_decay):\n",
        "    u_w = {}\n",
        "    u_b = {}\n",
        "    epsilon = 1e-6\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_u_w == {} and prev_u_b == {}:\n",
        "            u_w[i] = (dw[i] ** 2)\n",
        "            u_b[i] = (db[i] ** 2)\n",
        "        else:\n",
        "            u_w[i] = prev_u_w[i] + (dw[i] ** 2)\n",
        "            u_b[i] = prev_u_b[i] + (db[i] ** 2)\n",
        "\n",
        "        weights[i] -= learning_rate * dw[i] / (np.sqrt(u_w[i] + epsilon))\n",
        "        biases[i] -= learning_rate * db[i] / (np.sqrt(u_b[i] + epsilon))\n",
        "\n",
        "    return weights, biases, u_w, u_b\n",
        "\n",
        "def adadelta_gradient(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, prev_v_w, prev_v_b, weight_decay):\n",
        "    u_w = {}\n",
        "    u_b = {}\n",
        "    v_w = {}\n",
        "    v_b = {}\n",
        "    beta = 0.9\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_v_b == {} and prev_v_w == {}:\n",
        "            v_w[i] = (1 - beta) * (dw[i] ** 2)\n",
        "            v_b[i] = (1 - beta) * (db[i] ** 2)\n",
        "            update_w = (np.sqrt(epsilon) / np.sqrt(v_w[i] + epsilon)) * dw[i]\n",
        "            update_b = (np.sqrt(epsilon) / np.sqrt(v_b[i] + epsilon)) * db[i]\n",
        "            weights[i] -= update_w\n",
        "            biases[i] -= update_b\n",
        "            u_w[i] = (1 - beta) * (update_w ** 2)\n",
        "            u_b[i] = (1 - beta) * (update_b ** 2)\n",
        "        else:\n",
        "            v_w[i] = beta * prev_v_w[i] + (1 - beta) * (dw[i] ** 2)\n",
        "            v_b[i] = beta * prev_v_b[i] + (1 - beta) * (db[i] ** 2)\n",
        "            update_w = (np.sqrt(prev_u_w[i] + epsilon) / np.sqrt(v_w[i] + epsilon)) * dw[i]\n",
        "            update_b = (np.sqrt(prev_u_b[i] + epsilon) / np.sqrt(v_b[i] + epsilon)) * db[i]\n",
        "            weights[i] -= update_w\n",
        "            biases[i] -= update_b\n",
        "            u_w[i] = beta * prev_u_w[i] + (1 - beta) * (update_w ** 2)\n",
        "            u_b[i] = beta * prev_u_b[i] + (1 - beta) * (update_b ** 2)\n",
        "\n",
        "    return weights, biases, u_w, u_b, v_w, v_b\n",
        "\n",
        "def adam_gradient(dw, db, weights, biases, learning_rate, prev_m_w, prev_m_b, prev_v_w, prev_v_b, weight_decay, iteration):\n",
        "    m_w = {}\n",
        "    m_b = {}\n",
        "    v_w = {}\n",
        "    v_b = {}\n",
        "\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_m_w == {} and prev_m_b == {}:\n",
        "            m_w[i] = (1 - beta1) * dw[i]\n",
        "            m_b[i] = (1 - beta1) * db[i]\n",
        "            v_w[i] = (1 - beta2) * (dw[i] ** 2)\n",
        "            v_b[i] = (1 - beta2) * (db[i] ** 2)\n",
        "        else:\n",
        "            m_w[i] = beta1 * prev_m_w[i] + (1 - beta1) * dw[i]\n",
        "            m_b[i] = beta1 * prev_m_b[i] + (1 - beta1) * db[i]\n",
        "            v_w[i] = beta2 * prev_v_w[i] + (1 - beta2) * (dw[i] ** 2)\n",
        "            v_b[i] = beta2 * prev_v_b[i] + (1 - beta2) * (db[i] ** 2)\n",
        "        m_w_hat = m_w[i] / (1 - beta1**iteration)\n",
        "        m_b_hat = m_b[i] / (1 - beta1**iteration)\n",
        "        v_w_hat = v_w[i] / (1 - beta2**iteration)\n",
        "        v_b_hat = v_b[i] / (1 - beta2**iteration)\n",
        "        weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "        biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    return weights, biases, m_w, m_b, v_w, v_b\n",
        "\n",
        "def nadam_gradient(dw, db, weights, biases, learning_rate, prev_m_w, prev_m_b, prev_v_w, prev_v_b, weight_decay, iteration):\n",
        "    m_w = {}\n",
        "    m_b = {}\n",
        "    v_w = {}\n",
        "    v_b = {}\n",
        "\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        dw[i] += weight_decay * weights[i]\n",
        "        if prev_m_w == {} and prev_m_b == {}:\n",
        "            m_w[i] = (1 - beta1) * dw[i]\n",
        "            m_b[i] = (1 - beta1) * db[i]\n",
        "            v_w[i] = (1 - beta2) * (dw[i] ** 2)\n",
        "            v_b[i] = (1 - beta2) * (db[i] ** 2)\n",
        "        else:\n",
        "            m_w[i] = beta1 * prev_m_w[i] + (1 - beta1) * dw[i]\n",
        "            m_b[i] = beta1 * prev_m_b[i] + (1 - beta1) * db[i]\n",
        "            v_w[i] = beta2 * prev_v_w[i] + (1 - beta2) * (dw[i] ** 2)\n",
        "            v_b[i] = beta2 * prev_v_b[i] + (1 - beta2) * (db[i] ** 2)\n",
        "        m_w_hat = m_w[i] / (1 - beta1**iteration)\n",
        "        m_b_hat = m_b[i] / (1 - beta1**iteration)\n",
        "        v_w_hat = v_w[i] / (1 - beta2**iteration)\n",
        "        v_b_hat = v_b[i] / (1 - beta2**iteration)\n",
        "        lookahead_m_w = beta1 * m_w_hat + (1 - beta1) * dw[i] / (1 - beta1 ** iteration)\n",
        "        lookahead_m_b = beta1 * m_b_hat + (1 - beta1) * db[i] / (1 - beta1 ** iteration)\n",
        "        weights[i] -= learning_rate * lookahead_m_w / (np.sqrt(v_w_hat) + epsilon)\n",
        "        biases[i] -= learning_rate * lookahead_m_b / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    return weights, biases, m_w, m_b, v_w, v_b\n",
        "\n",
        "def validation(x_val, y_val, weights, biases, activation_func, n_hidden, loss_function):\n",
        "    val_loss_final = 0\n",
        "    y_pred_val = []\n",
        "    y_val_j = []\n",
        "    for j in range(0, len(x_val)):\n",
        "        x_val_each = x_val[j]\n",
        "        y_val_each = y_val[j]\n",
        "        activ, _,val_loss = forward_pass(x_val_each, y_val_each, weights, biases, activation_func, n_hidden, loss_function)\n",
        "        a_1 = activ[-1]\n",
        "        y_pred_val.append(np.argmax(a_1))\n",
        "        y_val_j.append(np.argmax(y_val[j]))\n",
        "        val_loss_final = val_loss_final + val_loss\n",
        "    accuracy = np.mean(np.array(y_pred_val) == np.array(y_val_j))\n",
        "    return val_loss_final/len(x_val), accuracy\n",
        "\n",
        "def test(weights, biases, activation_func, n_hidden, loss_function, plot=True):\n",
        "    test_loss_final = 0\n",
        "    y_pred_test = []\n",
        "    y_test_j = np.argmax(Y_test, axis=1)\n",
        "\n",
        "    for j, (x_test_each, y_test_each) in enumerate(zip(X_test, Y_test)):\n",
        "        activ, _, test_loss = forward_pass(x_test_each, y_test_each, weights, biases, activation_func, n_hidden, loss_function)\n",
        "        y_pred_test.append(np.argmax(activ[-1]))\n",
        "        test_loss_final += test_loss  # Accumulate loss\n",
        "\n",
        "    accuracy = np.mean(np.array(y_pred_test) == y_test_j)\n",
        "    wandb.log({\"Test Accuracy\": accuracy})\n",
        "    return test_loss_final/len(X_test), accuracy\n",
        "\n",
        "def Neuralnet(x_train, y_train, x_val, y_val, n_hidden, n_neurons_hidden, epochs, batch_size, activation, optimization, learning_rate, weight_decay, loss_function, weight_initialization):\n",
        "    x_train = input_layer(x_train)\n",
        "    y_train = one_hot_encode(y_train, classes)\n",
        "    x_val = input_layer(x_val)\n",
        "    y_val = one_hot_encode(y_val, classes)\n",
        "    features = x_train.shape[1]\n",
        "    num_neurons = [features] + [n_neurons_hidden]*(n_hidden) + [classes]\n",
        "    initialize_weights = {\"random\": initialize_weights_random, \"xavier\": initialize_weights_xavier}[weight_initialization]\n",
        "    activation_func = {\"sigmoid\": sigmoid, \"tanh\": tanh, \"relu\": Relu}[activation]\n",
        "    activation_deriv = {\"sigmoid\": der_sigmoid, \"tanh\": der_tanh, \"relu\": der_Relu}[activation]\n",
        "    optimization_func = {\"momentum\": momentum_gradient, \"sgd\": gradient_descent, \"nestrov\": nestrov_gradient, \"rmsprop\": rmsprop_gradient, \"adagrad\": adagrad_gradient, \"adadelta\": adadelta_gradient, \"adam\": adam_gradient, \"nadam\": nadam_gradient}[optimization]\n",
        "    loss_function = {\"bce\": bce_loss_function, \"mse\": mse_loss_function}[loss_function]\n",
        "    weights, biases = initialize_weights(num_neurons)\n",
        "\n",
        "    if optimization_func == gradient_descent:\n",
        "        #val_loss, val_accuracy = validation(x_val, y_val, weights, biases, activation_func, n_hidden, loss_function)\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(x_train), batch_size):\n",
        "                x_batch = x_train[i:i + batch_size]\n",
        "                y_batch = y_train[i:i + batch_size]\n",
        "                dw = {}\n",
        "                db = {}\n",
        "                for x,y in zip(x_batch,y_batch):\n",
        "                    activations, pre_activations,_ = forward_pass(x, y, weights, biases, activation_func, n_hidden, loss_function)\n",
        "                    del_L_w, del_L_b = back_propagation(activations, pre_activations, weights, biases, x, y, activations[-1], n_hidden, activation_deriv, loss_function)\n",
        "                    for key,value in del_L_w.items():\n",
        "                        if key not in dw:\n",
        "                            dw[key] = value\n",
        "                        else:\n",
        "                            dw[key] = dw[key] + value\n",
        "                    for key,value in del_L_b.items():\n",
        "                        if key not in db:\n",
        "                            db[key] = value\n",
        "                        else:\n",
        "                            db[key] = db[key] + value\n",
        "                for key in dw:\n",
        "                    dw[key] /= batch_size\n",
        "                    db[key] /= batch_size\n",
        "                weights, biases = optimization_func(dw, db, weights, biases, learning_rate, weight_decay)\n",
        "\n",
        "    elif optimization_func == nestrov_gradient:\n",
        "        #val_loss, val_accuracy = validation(x_val, y_val, weights, biases, activation_func, n_hidden, loss_function)\n",
        "        prev_u_w = {}\n",
        "        prev_u_b = {}\n",
        "        beta = 0.9\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(x_train), batch_size):\n",
        "                x_batch = x_train[i:i + batch_size]\n",
        "                y_batch = y_train[i:i + batch_size]\n",
        "                dw = {}\n",
        "                db = {}\n",
        "                for x,y in zip(x_batch,y_batch):\n",
        "                    if i == 0 and epoch == 0:\n",
        "                        activations, pre_activations,_ = forward_pass(x, y, weights, biases, activation_func, n_hidden, loss_function)\n",
        "                        del_L_w, del_L_b = back_propagation(activations, pre_activations, weights, biases, x, y, activations[-1], n_hidden, activation_deriv, loss_function)\n",
        "                    else:\n",
        "                        look_ahead_weights = {key: weights[key] - beta*prev_u_w[key] for key in range(len(weights))}\n",
        "                        look_ahead_biases = {key: biases[key] - beta*prev_u_b[key] for key in range(len(weights))}\n",
        "                        activations, pre_activations,_ = forward_pass(x, y, look_ahead_weights, look_ahead_biases, activation_func, n_hidden, loss_function)\n",
        "                        del_L_w, del_L_b = back_propagation(activations, pre_activations, look_ahead_weights, look_ahead_biases, x, y, activations[-1], n_hidden, activation_deriv, loss_function)\n",
        "                    for key,value in del_L_w.items():\n",
        "                        if key not in dw:\n",
        "                            dw[key] = value\n",
        "                        else:\n",
        "                            dw[key] = dw[key] + value\n",
        "                    for key,value in del_L_b.items():\n",
        "                        if key not in db:\n",
        "                            db[key] = value\n",
        "                        else:\n",
        "                            db[key] = db[key] + value\n",
        "                for key in dw:\n",
        "                    dw[key] /= batch_size\n",
        "                    db[key] /= batch_size\n",
        "                weights, biases, prev_u_w, prev_u_b = optimization_func(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, weight_decay)\n",
        "\n",
        "    elif optimization_func == momentum_gradient or optimization_func == rmsprop_gradient or optimization_func == adagrad_gradient or optimization_func == adadelta_gradient or optimization_func == adam_gradient or optimization_func == nadam_gradient:\n",
        "        #val_loss, val_accuracy = validation(x_val, y_val, weights, biases, activation_func, n_hidden, loss_function)\n",
        "        if optimization_func == adadelta_gradient or optimization_func == adam_gradient or optimization_func == nadam_gradient:\n",
        "            iteration = 0\n",
        "            prev_u_w = {}\n",
        "            prev_u_b = {}\n",
        "            prev_v_w = {}\n",
        "            prev_v_b = {}\n",
        "        else:\n",
        "            prev_u_w = {}\n",
        "            prev_u_b = {}\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(x_train), batch_size):\n",
        "                x_batch = x_train[i:i + batch_size]\n",
        "                y_batch = y_train[i:i + batch_size]\n",
        "                dw = {}\n",
        "                db = {}\n",
        "                for x,y in zip(x_batch,y_batch):\n",
        "                    activations, pre_activations,_ = forward_pass(x, y, weights, biases, activation_func, n_hidden, loss_function)\n",
        "                    del_L_w, del_L_b = back_propagation(activations, pre_activations, weights, biases, x, y, activations[-1], n_hidden, activation_deriv, loss_function)\n",
        "                    for key,value in del_L_w.items():\n",
        "                        if key not in dw:\n",
        "                            dw[key] = value\n",
        "                        else:\n",
        "                            dw[key] = dw[key] + value\n",
        "                    for key,value in del_L_b.items():\n",
        "                        if key not in db:\n",
        "                            db[key] = value\n",
        "                        else:\n",
        "                            db[key] = db[key] + value\n",
        "                for key in dw:\n",
        "                    dw[key] /= batch_size\n",
        "                    db[key] /= batch_size\n",
        "                if optimization_func == adadelta_gradient:\n",
        "                    weights, biases, prev_u_w, prev_u_b, prev_v_w, prev_v_b = optimization_func(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, prev_v_w, prev_v_b, weight_decay)\n",
        "                elif optimization_func == adam_gradient or optimization_func == nadam_gradient:\n",
        "                    iteration +=1\n",
        "                    weights, biases, prev_u_w, prev_u_b, prev_v_w, prev_v_b = optimization_func(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, prev_v_w, prev_v_b, weight_decay, iteration)\n",
        "                else:\n",
        "                    weights, biases, prev_u_w, prev_u_b = optimization_func(dw, db, weights, biases, learning_rate, prev_u_w, prev_u_b, weight_decay)\n",
        "    test(weights, biases, activation_func, n_hidden, loss_function, plot = True)\n"
      ],
      "metadata": {
        "id": "i2jRBA7Mhxrd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = input_layer(X_test)\n",
        "Y_test = one_hot_encode(Y_test, classes)"
      ],
      "metadata": {
        "id": "ZxUnky6kcXrd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"optimizer\": {\"values\": [\"adam\", \"nadam\"]},\n",
        "        \"epochs\": {\"values\": [10]},\n",
        "        \"num_layers\": {\"values\": [3, 4]},\n",
        "        \"num_neurons_hidden\": {\"values\": [128]},\n",
        "        \"batch_size\": {\"values\": [32]},\n",
        "        \"learning_rate\": {\"values\": [0.001]},\n",
        "        \"weight_decay\": {\"values\": [0]},\n",
        "        \"loss_function\": {\"values\": [\"bce\"]},\n",
        "        \"activation\": {\"values\": [\"tanh\"]},\n",
        "        \"weight_initialization\": {\"values\": [\"xavier\"]}\n",
        "    },\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Fashion-mnist\")\n",
        "def train():\n",
        "    try:\n",
        "        wandb.init(group = \"mnist\")\n",
        "        config = wandb.config\n",
        "        optimizer = config.optimizer\n",
        "        epochs = config.epochs\n",
        "        n_hidden = config.num_layers\n",
        "        n_neurons_hidden = config.num_neurons_hidden\n",
        "        batch_size = config.batch_size\n",
        "        learning_rate = config.learning_rate\n",
        "        weight_decay = config.weight_decay\n",
        "        loss_function = config.loss_function\n",
        "        activation_str = config.activation\n",
        "        weight_initialization = config.weight_initialization\n",
        "\n",
        "        Neuralnet(x_train_final, y_train_final, x_val_final, y_val_final, n_hidden, n_neurons_hidden, epochs, batch_size, activation_str, optimizer, learning_rate, weight_decay, loss_function, weight_initialization)\n",
        "    finally:\n",
        "        wandb.finish()\n",
        "wandb.agent(sweep_id, train, count=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y6mG76PLs-Jp",
        "outputId": "55dbfd8c-ddf4-4ef6-e5ac-750ebb2ad93b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 09wsmpoz\n",
            "Sweep URL: https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ng96oq7i with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: bce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_hidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_161310-ng96oq7i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/ng96oq7i' target=\"_blank\">playful-sweep-1</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/ng96oq7i' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/ng96oq7i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.9681</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">playful-sweep-1</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/ng96oq7i' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/ng96oq7i</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_161310-ng96oq7i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: en4m1b5w with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: bce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_hidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_162327-en4m1b5w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/en4m1b5w' target=\"_blank\">cool-sweep-2</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/en4m1b5w' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/en4m1b5w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.9724</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cool-sweep-2</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/en4m1b5w' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/en4m1b5w</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_162327-en4m1b5w/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qoyp3cpe with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: bce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_hidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_163401-qoyp3cpe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/qoyp3cpe' target=\"_blank\">elated-sweep-3</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/qoyp3cpe' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/qoyp3cpe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.9658</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">elated-sweep-3</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/qoyp3cpe' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/qoyp3cpe</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_163401-qoyp3cpe/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lv61wa6k with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: bce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_hidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_164555-lv61wa6k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/lv61wa6k' target=\"_blank\">fine-sweep-4</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/sweeps/09wsmpoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/lv61wa6k' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/lv61wa6k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.9695</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-sweep-4</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/lv61wa6k' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist/runs/lv61wa6k</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/Fashion-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_164555-lv61wa6k/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}